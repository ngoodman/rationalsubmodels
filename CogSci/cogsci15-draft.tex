% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified: Niels Taatgen (taatgen@cmu.edu) 10/24/2006

%% Change ``a4paper'' in the following line to ``letterpaper'' if you are
%% producing a letter-format document.

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{bbm,amsmath,amssymb}
\usepackage{pslatex}
\usepackage{natbib}
\usepackage{bm}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{verbatim}
\newcommand{\tb}[1]{\textcolor{Blue}{#1}}
%\usepackage{apacite}


\title{Reasoning Reasonably by Ignoring Causal Knowledge}
 %NDG: ...or something else?
 
 
\author{{\large \bf Noah D. Goodman} (ngoodman@stanford.edu), {\large \bf Thomas F. Icard, III} (icard@stanford.edu) \\
  Departments of Psychology and Philosophy, Stanford University}
  %NDG: you can be first if you want, since you did the main writing. or we could do a ``contributed equally'' note.

\begin{document}

\maketitle


\begin{abstract}
Blah blah.. \vspace{2.2in}

\textbf{Keywords:} 
frame problem, bounded-resource-rationality, causal reasoning, alternative neglect.
\end{abstract}

\section{Introduction}

To any inference or decision problem, there is no \emph{a priori} bound on what aspects of a person's knowledge may be usefully, or even critically, applied. In principle, anything could be related to anything. 
This challenge is sometimes referred to as the \emph{frame problem}, characterized by \cite{Glymour1987} as: ``Given an enormous amount of stuff, and some task to be done using some of the stuff, what is the \emph{relevant stuff} for the task?'' (65). The question is foundational to reasoning and rationality. 
%NDG: should we introduce the causal frame problem explicitly here? or down a bit?
Part of what makes people so smart is their ability to solve the frame problem, ignoring those aspects of the world (and their knowledge of it) that are irrelevant to the problem at hand, thereby simplifying the underlying reasoning task, turning an intractable problem into a tractable one.

Not all of the psychological literature paints a picture of human reasoners as so efficient, however. In the literature on causal reasoning, there is a robust empirical finding that subjects often neglect causal variables, including those that are in principle accessible to the subject, which would sometimes allow the subject to make better, more accurate inferences. So called \emph{alternative neglect} is an especially well documented phenomenon, in which subjects ignore alternative possible causes of some event (\citealt{Fischhoff1978,KlaymanHa,Fernbach2011}, \emph{inter alia}), even when doing so leads to incorrect inferences, and would evidently not require a significant  amount of additional effort. More generally, at least in the causal domain, subjects tend to consider ``smaller'' models of the world than would be relevant to the task at hand, given the subject's knowledge and reasoning abilities. This has led many to criticize the behavior as normatively objectionable. Perhaps people are ignoring too much of their knowledge.

We would like to suggest that alternative neglect and related phenomena may be natural consequences of a general mechanism for sifting the most pertinent information from all other knowledge---that is, for solving the frame problem. Perhaps the mind tolerates local and occasional inaccuracy in order to achieve a more global efficiency. 
%NDG: need to introduce the causal frame problem and the idea of a sub-model before here.
To substantiate this claim, we need a better understanding of what it is for a submodel to be more or less apt for a task, from the perspective of a reasoner with bounded time and resources. What is a \emph{(resource-)rational submodel}? It is clear that human reasoners cannot consult an indefinitely detailed mental model of the world for every inference task. So what kind of simpler model \emph{should} a reasoner consult for a given task?

This work follows a line of research in cognitive science concerned with \emph{bounded} or \emph{resource rationality} (\citealt{Simon1957,Gigerenzer1996}, \emph{inter alia}), and specifically in the context of probabilistic models, and approximations thereto \citep{Vul2014}. In addition to inherent interest, it has recently been suggested that considerations of bounded rationality may play a methodological role in sharpening the search for reasonable accounts of the cognitive processes underlying inductive inference \citep{Griffiths2014,Icard2014}. However, in this tradition there has been a focus on the algorithm used for inference in a given model, and less attention paid to questions of model selection.

In this paper we offer a framework for solving the causal frame problem by selecting rational submodels, provide several illustrative examples, and address some of the empirical findings concerning alternative neglect.

\section{Resource-Rational Submodels}

Let $P(\textbf{X})$ be a joint probability distribution over random variables $\textbf{X} = X_1,X_2,\dots$, and define a \emph{query} to be a partition $\langle \textbf{X}_q;\textbf{X}_l;\textbf{X}_e\rangle$ of \textbf{X} into \emph{query variables}, \emph{latent variables}, and \emph{evidence variables}, respectively. A typical query task is to find values of $\textbf{X}_q$ that maximize the conditional probability $P(\textbf{X}_q\mid \textbf{X}_e = \textbf{v})$, marginalizing over values of $\textbf{X}_l$. Clearly, the difficulty of this and related tasks scales with the number of variables. We will be interested in smaller models with fewer variables: a sublist $\textbf{X}^*$ of $\textbf{X}$ with associated distribution $P^*(\textbf{X}^*)$, and associated partition $\langle \textbf{X}_q;\textbf{X}_l^*;\textbf{X}_e^*\rangle$, so that only latent and evidence variables are ignored. The canonical way of choosing $P^*$ given $\textbf{X}^*$ is as to treat all neglected variables as though they are uniform, effectively ignoring any (direct or indirect) influence they may have on $\textbf{X}_q$.

%NDG: we should say something about why these two questions are of interest?
Given $P(\textbf{X})$ and $P^*(\textbf{X}^*)$, there are at least two kinds of questions we would like to ask:\footnote{Strictly speaking, the second can be seen a special case of the first, with a logarithmic scoring rule \citep{Bernardo}.} \begin{enumerate}
  \item Given a decision problem with action space $\mathcal{A}$ and utility function $U:\textbf{X}_q\times \mathcal{A} \rightarrow\mathbb{R}$, and assuming fixed a (stochastic) choice rule $\Psi_Q$ taking a distribution $Q$ over $\textbf{X}_q$ to a distribution on actions, what are the respective expected utilities of using $P$ and $P^*$ under (assumed ``true'') distribution $P$? That is, how great is the following difference $\Delta_{P,P^*}$? \end{enumerate}
  $$\Delta_{P,P^*} \;\; = \;\; \mathbb{E}_{\textbf{x}\sim P}\;\mathbb{E}_{A \sim \Psi_{P}}\;U(\textbf{x},A) \,-\, \mathbb{E}_{\textbf{x} \sim P}\;\mathbb{E}_{A \sim \Psi_{P^*}}\;U(\textbf{x},A)$$
  \begin{enumerate}
  \item[2.] How far is $P^*$ from $P$ in information distance, for the variables $\textbf{X}_q$ of interest? That is, what is the Kullback-Leibler divergence between $P$ and $P^*$ with respect to $\textbf{X}_q$?
\end{enumerate}$$KL(P \;||\; P^*)  =  \sum_{\textbf{x}} P(\textbf{X}_q = \textbf{x} \mid \textbf{X}_{e} = \textbf{v})\; \mbox{log} \; \frac{P(\textbf{X}_q = \textbf{x} \mid \textbf{X}_{e} = \textbf{v})}{P^*(\textbf{X}_q = \textbf{x} \mid \textbf{X}^*_{e} = \textbf{v}^*)}$$ In general we will expect that $KL(P \;||\; P^*) > 0$, and $\mathbb{E}_{\textbf{x}\sim P}\;\mathbb{E}_{A \sim \Psi_{P}}\;U(\textbf{x},A) > \mathbb{E}_{\textbf{x} \sim P}\;\mathbb{E}_{A \sim \Psi_{P^*}}\;U(\textbf{x},A)$. However, in line with other work on resource rationality, assuming the requisite computations using distribution $P$ come with a greater cost than when using $P^*$, this difference in cost may well be worth the difference in accuracy or utility. Given a cost function on approximations $P^*$, one can then also ask comparative questions about alternative approximations.

%NDG: let's be more explicit about the resource-optimizing submodel here, so that we can pose the question how much of a model might it be rational to ignore?

%NDG: hammer home the point that the HMM is an infinite model with all nodes potentially mattering -- i.e. the essential frame problem -- but the resource rational analysis makes it very finite.

In what follows we illustrate these ideas with three examples using familiar graphical models. The first example, of an HMM, demonstrates a clear case where information loss in the submodel is sufficiently low that any cost at all would merit neglect of most variables. The second example, of a causal Bayes net, shows that under a sampling scheme for decision making \citep{Vul2014}, the submodel actually outperforms the ``ideal'' full model in many cases. Finally, the third example reveals that certain kinds of inferences may be subject to greater information loss resulting from alternative neglect than others. Recent empirical literature shows that people respect this difference, suggesting that there may indeed be an element of resource rationality in neglect behavior.

\section{Hidden Markov Models}

A Hidden Markov Model is given by a time-labeled sequence of state variables $X_1,X_2,\dots$, with transition probability $P(X_{t+1}\mid X_t)$, and a sequence of evidence variables $Y_1,Y_2,\dots$, with emission probabilities $P(Y_t\mid X_t)$. In a typical inference task, after observing some values of $Y$ (blue), we are interested in the value of $X_{t+1}$ (beige) at time $t+1$:

\begin{figure}[h] 
\begin{center}
  \begin{tikzpicture}
  
  
  \node (s0) at (-.25,1.25)  {$\textbf{\dots}$};
  
  \node (s1) at (1,1.25) [circle,draw=black,minimum size=0.55cm] {};

  \node (s2) at (1,0) [circle,draw=black,fill=ProcessBlue,minimum size=0.55cm] {};
  
  \node (s3) at (2.25,1.25) [circle,draw=black,minimum size=0.55cm] {};
  
  \node (s4) at (2.25,0)  [circle,draw=black,fill=ProcessBlue,minimum size=0.55cm] {};
  
  \node (s5) at (3.5,1.25) [circle,draw=black,minimum size=0.55cm] {};
  
  \node (s6) at (3.5,0)  [circle,draw=black,fill=ProcessBlue,minimum size=0.55cm] {};
  
  \node (s7) at (4.75,1.25) [circle,draw=black,minimum size=0.55cm,fill=Apricot] {};
 
  \path (s0) edge[->] (s1);
  
  \path (s1) edge[->] (s2);
  
  \path (s1) edge[->] (s3);
  
  \path (s3) edge[->] (s4);
  
  \path (s3) edge[->] (s5);
  
  \path (s5) edge[->] (s6);
  
  \path (s5) edge[->] (s7);
  
 \end{tikzpicture}
\end{center} 
\end{figure} 
\noindent For instance, variables $X$ might be whether there is high or low air pressure, while observations $Y$ are of sun or clouds. While in principle determining $X_{t+1}$---what the weather will be like tomorrow---could depend on indefinitely earlier observations and states, one has the intuition that ``looking back'' only a few steps should be sufficient for typical purposes.

Indeed, we can calculate the KL-divergence for particular example HMMs, to determine how far off an estimate will be by truncating at various points. When the model is truncated at variable $X_{t-N}$, we assume the distribution $P^{*}(X_{t-N})$ is uniform.  In Figure \ref{hmm} is a graph showing KL between the full model and a submodel with only $N$ previous time steps.
%NDG: we need to say what this particular HMM is (two states, what transitions). also maybe give a sense of the KL curve as a function of the ``stiffness'' (second eigenvalue?)  of the transition function. plot stiffness vs optimal truncation depth?

\begin{center} \begin{figure}[h]
\includegraphics[scale=0.42]{hmm.png} \caption{KL values for illustrative HMM with all binary variables, and $P(X_{t+1}\mid X_t) = P(Y_t\mid X_t) = 0.8$.} \label{hmm}
\end{figure} 
\end{center}
\texttt{Some further analysis of the HMM example is probably called for. Perhaps some EU calculations, or other KL calculations? I suppose we are treating this as a `warm-up'? Also, as below, I think the model here uses log base $e$. I used base 2 in my calculations, so we should be consistent one way or the other.}

\section{Neglecting Alternative Causes}

Consider next a simple causal model under a Noisy-Or parameterization \citep{Cheng}. That is, suppose we have binary causal variables $\textbf{X}$, taking on values 0 or 1, and conditional probabilities given in terms of weights $\theta_{Y,X}$ codifying the influence of parent $Y$ on a variable $X$, and a ``background bias'' parameter $\beta$: $$P\big(X\mid \textbf{pa}(X)\big) \quad = \quad 1-\Big((1-\beta)\prod_{Y \in \textbf{pa}(X)} (1-\theta_{Y,X})^y\Big)$$
Suppose in particular we have variables $A,B,C,D$ with weights $\theta_1, \theta_2$, and $\theta_3$, as depicted on the left in Figure \ref{noisy-or}.
\begin{figure}[h] 
\begin{center}
  \begin{tikzpicture}
  
  \node (s0) at (.5,1.25) [circle,draw=black] {$C$};
  
  \node (s1) at (1.5,1.25) [circle,draw=black] {$A$};
  
  \node (s2) at (0,0) [circle,draw=black,fill=Apricot] {$B$};
  
  \node (s3) at (1,0) [circle,draw=black,fill=ProcessBlue] {$D$};
 
  \path (s0) edge[->] (s2);
  
  \path (s0) edge[->] (s3);
  
  \path (s1) edge[->] (s3);
  
  \node (i1) at (.1,.7) {$\theta_1$};
  
  \node (i1) at (.92,.7) {$\theta_2$};
  
  \node (i1) at (1.5,.6) {$\theta_3$};
  
 \end{tikzpicture} \hspace{0.7in}
   \begin{tikzpicture}
  
  \node (s0) at (.5,1.25) [circle,draw=black] {$C$};
  
  \node (s2) at (0,0) [circle,draw=black,fill=Apricot] {$B$};
  
  \node (s3) at (1,0) [circle,draw=black,fill=ProcessBlue] {$D$};
 
  \path (s0) edge[->] (s2);
  
  \path (s0) edge[->] (s3);
  
  \node (i1) at (.1,.7) {$\theta_1$};
  
  \node (i1) at (.92,.7) {$\theta_2$};
  
 \end{tikzpicture}
\end{center} \caption{Full Model versus Partial Submodel} \label{noisy-or}
\end{figure} 
Imagine, for instance, a case of social reasoning, in which: \begin{itemize} \item[] $A$: ``Mary has lost her usual gloves'' \item[] $B$: ``Mary has her bicycle with her''  \item[] $C$: ``Mary is going cycling"  \item[] $D$:  ``Mary is wearing cycling gloves'' \end{itemize}
Observing that Mary is wearing cycling gloves makes it more likely that she is going cycling, and therefore that she has her bike with her. But this is attenuated by the alternative possible cause, that she lost her other gloves. Our question in this case is, how much worse will a reasoner fare by ignoring alternative cause $A$, that is, by using the smaller submodel  in Figure \ref{noisy-or}, on the right?
%NDG: note that noisy-or models have the nice property that you can delete a variable and get a wellformed distribution without having to assume some other distribution (e.g. uniform) for that variable?

To assess this question, we can look at both the difference in expected utility and the KL-divergence between using the ``true'' distribution $P$ and the approximate distribution $P^*$, for $B$ given $D=1$. For the EU calculation, suppose our agent is making a guess based on a single sample from distributions $P$ and $P^*$,\footnote{See, e.g., \citep{Vul2014}. This assumption is more apt in more complicated models, where computing exact estimates would be harder. We consider this kind of rule simply for illustration, and contrast with information distance, which would be more closely aligned with an agent (non-noisily) maximizing expected utility.} 
%NDG: we should introduce the idea of using a submodel with an approximate inference algorithm earlier. we can there point out that the best submodel may be different depending on the (approximate) inference algorithm used, and set up the one-sample case. here we should first do the full conditional version, then the one-sample version.
and that utility 1 is achieved for a correct response, 0 for incorrect. Some example calculations are summarized below in Table \ref{table1}. \texttt{We will of course want to curtail this list!}

\begin{table}[h]  \begin{center}
\begin{tabular}{c | c | c | c | c | c | c}
 $P(C)$ & $P(A)$ & $\theta_1$ & $\theta_2$ & $\theta_3$ & $\Delta_{P,P^*}$ & $KL(P \;\vert\vert\; P^*)$ \\ \hline
 $0.5$ & $0.5$ & $1$ & $1$ & $1$ & $-0.097$ & $0.411$ \\
  $0.5$ & $0.5$ & $1$ & $0.5$ & $0.9$ & $-0.074$ & $0.324$ \\
  $0.5$ & $0.5$ &$1$ & $0.5$ & $0.5$ & $-0.087$ & $0.187$ \\
 $0.5$ & $0.5$ & $1$ & $0.9$ & $0.5$ & $-0.096$ & $0.181$ \\
 $0.5$ & $0.5$ & $0.9$ & $0.9$ & $0.9$ & $-0.057$ & $0.179$ \\
 $0.5$ & $0.5$ & $0.9$ & $0.5$ & $0.5$ & $-0.053$ & $0.104$ \\
 $0.5$ & $0.5$ &  $0.9$ & $0.1$ & $0.5$ & $-0.007$ & $0.056$ \\
 $0.5$ & $0.5$ & $0.3$ & $0.5$ & $0.9$ & $0.048$ & $0.019$ \\
 $0.5$ & $0.5$ & $0.5$ & $0.9$ & $0.3$ & $0.007$ & $0.007$ \\
 $0.5$ & $0.5$ & $0.1$ & $0.1$ & $0.1$ & $0.006$ & $0.000$ \\
 $0.3$ & $0.1$ & $1$ & $1$ & $1$ & $-0.073$ & $0.083$ \\
 $0.3$ & $0.1$ & $1$ & $0.5$ & $0.9$ & $-0.054$ & $0.081$ \\
 $0.3$ & $0.1$ & $1$ & $0.9$ & $0.5$ & $-0.051$ & $0.029$ \\
 $0.3$ & $0.1$ &$1$ & $0.5$ & $0.5$ & $-0.045$ & $0.034$ \\
 $0.3$ & $0.1$ & $0.9$ & $0.9$ & $0.9$ & $-0.045$ & $0.042$ \\
 $0.3$ & $0.1$ &  $0.9$ & $0.5$ & $0.5$ & $-0.029$ & $0.022$ \\
 $0.3$ & $0.1$ & $0.9$ & $0.1$ & $0.5$ & $0.011$ & $0.015$
\end{tabular} \end{center} \caption{EU and KL values, with $\beta = 0.05$. \texttt{(Note: the Church program used here is log base $e$!)}} \label{table1}
\end{table} \noindent We also calculated the average difference in EU over values of $P(C)$ and $P(A)$ at $0.1$ intervals ($0.1$, $0.2$, etc.), and $0.01$ intervals for $\theta_1,\theta_2,\theta_3$ (thus, over 7 million parameter settings). That average is $0.024$.

Evidently, when making a binary, sample-based decision, using the smaller submodel does not greatly reduce one's success probability. In fact, as shown in Table \ref{table1}, for many parameter settings the agent actually fares better by using the simpler model. Take the first parameter setting, for example. In this case $P(B=1\mid D=1) \approx 0.673$, whereas $P^*(B=1 \mid D=1) \approx 0.953$. The submodel drastically overestimates the probability of $B$ by ignoring the alternative cause, as reflected in the very high KL-divergence. However, insofar as the true probability is significantly above $0.5$, if the agent is going to make a decision by drawing a single sample from this distribution, such overestimation is advantageous, since the agent is more likely to choose the more probable outcome. This advantage is only compounded by the reduction in computational cost resulting from the simpler model.

To conclude this small case-study, while in many cases the inaccuracy, as measured by KL-divergence, is not excessive, in some cases it can be. Hence, if confidence in estimation is important, or if more fine-grained decisions are called for, using a submodel in this case may be detrimental. However, even in cases where the KL-divergence is high, the misrepresentation may be to the benefit of the agent, depending on the nature of the decision problem and the choice rule being invoked. We can tentatively conclude from this that alternative neglect---even when it results in less accurate judgments---can be resource rational. 

\section{Predictive versus Diagnostic Reasoning}

Resource-rational analysis of submodel choice and alternative neglect predicts these phenomena to occur, at least to a first approximation, when they would result in resource-rational choice and action, as outlined above. Is this prediction born out by the data on alternative neglect?

One of the more robust recent findings in the causal reasoning literature is that subjects tend to neglect alternatives to a much greater extent in \emph{predictive} reasoning than in \emph{diagnostic} reasoning \citep{Fernbach2011,Fernbach2013}. In the simplest case, suppose we have three variables $A,B,C$ as in Figure \ref{diagnostic}.
\begin{figure}[h] \begin{center}
\begin{tikzpicture}
  \node (s0) at (0,1.25) [circle,draw=black,fill=ProcessBlue] {$C$};
  
  \node (s2) at (0,0) [circle,draw=black,fill=Apricot] {$B$};
  
  \node (s3) at (1,1.25) [circle,draw=black] {$A$};
 
  \path (s0) edge[->] (s2);
  
  \path (s3) edge[->] (s2);
  
  \node (l1) at (-.2,.65) {$\theta_1$};
  
  \node (l1) at (.8,.6) {$\theta_2$};
  
 \end{tikzpicture}
\hspace{.7in}
  \begin{tikzpicture}
  
  \node (s0) at (0,1.25) [circle,draw=black,fill=Apricot] {$C$};
  
  \node (s2) at (0,0) [circle,draw=black,fill=ProcessBlue] {$B$};
  
  \node (s3) at (1,1.25) [circle,draw=black] {$A$};
 
  \path (s0) edge[->] (s2);
  
  \path (s3) edge[->] (s2);
  
    \node (l1) at (-.2,.65) {$\theta_1$};
  
  \node (l1) at (.8,.6) {$\theta_2$};
  
 \end{tikzpicture} \end{center}
 \caption{Predictive versus Diagnostic Inference} \label{diagnostic}
\end{figure}
The left diagram depicts a predictive inference, where effect $B$ is queried given evidence that cause $C$ is active. On the right is a diagnostic inference, where the the causal variable $C$ is queried given evidence $B$. In this simple model, we can fold $P(A)$ and $\theta_{B,A}$ into a single parameter $\theta_2$, so that $A$ effectively has prior probability 1, and the resulting conditional probabilities can be simplified to: 
$$P(B\mid C) \;= \; \theta_1 + \theta_2 - \theta_1\theta_2$$
$$P(C \mid B) \;= \; 1- \big(1-P(C)\big)\Big(\theta_2\;/\;\big(P(C)\theta_1 + \theta_2  - P(C)\big)\Big)$$
The finding in \cite{Fernbach2011} is that subjects routinely ignore variable $A$ in predictive inference tasks, and thereby consistently make low estimates of $P(B\mid C)$. In diagnostic inference tasks, however, subjects show sensitivity to strength and number of alternatives, consequently making more accurate judgments. Indeed, there is a longer reaction time for diagnostic than for predictive inferences, and only in the diagnostic condition is there dependency of reaction time on number of alternatives \citep{Fernbach2010}. \cite{Fernbach2013} verified that this asymmetry between diagnostic and predictive reasoning is robust; in particular, it not due to availability or memory limitations.

In other words, like in the previous example, subjects seem to be reasoning with a submodel, ignoring variable $A$, in the predictive case, but not in the diagnostic case. How detrimental could it be to neglect $A$? Table \ref{predictive} shows the differences in expected utility for various parameter settings---again with a single-sample-based agent---between the EU of using the true distribution and the approximate distribution (ignoring variable $A$), for both the predictive and diagnostic cases. 
\begin{table}[h]  \begin{center}
\begin{tabular}{c | c | c | c | c | c}
 $P(C)$ & $P(A)$ & $\theta_1$ & $\theta_2$ & $\Delta_{pred}$ & $\Delta_{diag}$ \\ \hline
 $0.1$ & $0.1$ &  $0.9$ & $0.9$ & $0.007$ & $0.034$ \\
 $0.1$ & $0.1$ &  $0.7$ & $0.7$ & $0.009$ & $0.035$ \\
 $0.1$ & $0.1$ &  $0.5$ & $0.5$ & $0.002$ & $0.035$ \\
 $0.1$ & $0.1$ &  $0.3$ & $0.3$ & $-0.006$ & $0.031$ \\
 $0.1$ & $0.3$ &  $0.3$ & $0.3$ & $-0.013$ & $0.093$ \\
 $0.1$ & $0.3$ &  $0.7$ & $0.7$ & $0.033$ & $0.174$ \\
 $0.1$ & $0.1$ &  $0.3$ & $0.7$ & $-0.001$ & $0.075$ \\
 $0.1$ & $0.1$ &  $0.7$ & $0.3$ & $0.004$ & $-0.001$ \\
 $0.3$ & $0.1$ &  $0.7$ & $0.7$ & $0.009$ & $-0.060$ \\
 $0.3$ & $0.3$ &  $0.9$ & $0.9$ & $0.022$ & $-0.042$ \\
 $0.9$ & $0.9$ &  $0.5$ & $0.5$ & $0.102$ & $-0.049$ \\
 $0.9$ & $0.9$ &  $0.9$ & $0.9$ & $0.074$ & $-0.065$ \\
 $0.1$ & $1.0$ & $0.9$ & $0.3$ & $0.025$ & $0.227$ \\
 $0.1$ & $0.9$ &  $0.7$ & $0.3$ & $0.045$ & $0.216$ \\
 $0.1$ & $1.0$ & $0.9$ & $0.1$ & $0.008$ & $0.045$ \\
  $0.1$ & $0.5$ &  $0.9$ & $0.1$ & $0.004$ & $0.014$
\end{tabular} \end{center} \caption{Differences in expected utility between true and approximate distributions, predictive and diagnostic inferences. \texttt{Again, this can be drastically shortened. It also needs to be rewritten without $P(A)$.}} \label{predictive}
\end{table} 
In many cases, $\Delta_{pred}$ is indeed smaller than $\Delta_{pred}$, meaning that the agent suffers less in expected utility when using the smaller submodel. However, there are also many cases where $\Delta_{pred}$ is quite large, e.g., $\approx 0.1$, and at any rate greater than $\Delta_{diag}$. Indeed, the average $\Delta_{pred}$ value, over $0.01$ intervals for all parameters, is $0.198$---quite large, and significantly greater than $\Delta_{diag}$, whose average is $0.044$. Moreover, \cite{Fernbach2013} have shown that people exhibit neglect even in these cases where the mistake it rather serious. Thus, if we consider the single-sampling agent, and average over all parameter settings, it would seem that people ignore alternatives in the \emph{altogether wrong} cases.

Looking at KL-divergence, however, we see a different picture. In fact, without a background bias term (as in the previous example), for the diagnostic case ignoring variable $A$ will lead to the conclusion that $C$ has probability 1, which means the KL-divergence is not even defined. With a positive bias term, we can make the KL-divergence as large as we like, by making the bias smaller. For instance, with $\beta = 0.01$, the average value of $KL(P\;||\;P^*)$ is already $1.740$ in the diagnostic case. By contrast, it is only $0.298$ in the predictive case. 

If we take resource rationality as a working methodological hypothesis, this would suggest at least two conclusions. First, in simple situations like in Figure \ref{diagnostic}, people may not make decisions based on a single sample, which is after all a strategy that makes more sense in computationally demanding tasks---they may either take more samples, or use a different algorithmic strategy altogether. Relatedly, people may often face decision problems that call for more fine-grained judgments: not just which outcome is more likely, but how much more likely. For such tasks, KL-divergence matters.

Second, the results of \cite{Fernbach2011}, together with the proposed analysis, strongly suggest that people may optimize grain of representation only at a high level of abstraction, in terms of general features such as the type of inference in question (predictive versus diagnostic). Understanding how this might work will require expanding the analysis, e.g., to incorporate elements of metalevel control \citep{Icard2014,Lieder2014}, and further experimental work to understand under what conditions, and to what extent, alternatives are ignored.

\section{Further Questions and Directions}

%NDG: point out that this leaves open the problem of optimizing the params in the moment....

Dynamic view: Could consider a probabilistic generalization of the ``contradiction hypothesis" of Park \& Sloman (2013), concerning when to ``add'' a new causal variable. \cite{KlaymanHa}: people go with a hypothesis until it is contradicted by data. Confirmation only adds to it.

Tversky \& Koehler (1994) showed that when alternatives are ``unpacked'' increases its probability. Sloman et al. (2004) showed that if they are unpacked with atypical causes, then probability decreases!

Kahneman \& Tversky (1983) showed people judge cause+effect to be more likely than effect alone. Perhaps draw a single variable at first and (for whatever reason) underestimate; however, they can be made to consider larger causal structures, which may influence their judgment. 

Previous work on ``relevance'' of variables in graphical models \citep{Druzdzel}


More general class of approximation strategies: combining with MCMC strategies, etc. \citep{WickMcCallum}

``The frame problem goes very deep; it goes as deep as the analysis of rationality" \citep{Fodor1987}

%NDG: should do joint optimization of samples and submodel.

\bibliographystyle{apalike}

\bibliography{bayes}


\end{document}
